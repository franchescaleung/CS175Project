{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Chloebot</center></h1>\n",
    "\n",
    "<table><tr style=\"background:transparent;\">\n",
    "<td><img width=\"200\" height=\"200\" src=\"https://csml.princeton.edu/sites/csml/files/styles/pwds_featured_image/public/events/share.png\"></td>\n",
    "<td><img width=\"200\" height=\"200\" src=\"https://venturebeat.com/wp-content/uploads/2019/06/pytorch.jpg\"></td>\n",
    "<td><img width=\"100\" height=\"100\" src=\"https://avatars3.githubusercontent.com/u/56938552?s=100&v=1\"></td>  \n",
    "</tr></table>\n",
    "\n",
    "\n",
    "This tutorial is based on the research [Attention Is All You Need](https://arxiv.org/abs/1706.03762) from Google AI, and the PyTorch implementation by [Harvard NLP group](http://nlp.seas.harvard.edu/2018/04/03/attention.html) and [SamLynnEvans](https://github.com/SamLynnEvans/Transformer) \n",
    "\n",
    "Each of these rectangles, like this one you are reading from, and the ones with code in them, are called a cells, click one cell and Press *shift* + *return or enter* together, or go to Cell in the nav bar and click \"Run Cells\" to run each of the next cells below to summon chloe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from the the Dwightbot team:  The transformer was trained in this notebook by putting the edited corpus into the saved/custompairs.json file.  After reaching a good training cutoff the transformer weights were copied for usage with the talkdwight.py standalone code (which was mostly adapted from the methods taught in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jenet\\Documents\\CSJupyter\\chat-transformer-main\n",
      "C:\\Users\\jenet\\anaconda3\\python37.zip\n",
      "C:\\Users\\jenet\\anaconda3\\DLLs\n",
      "C:\\Users\\jenet\\anaconda3\\lib\n",
      "C:\\Users\\jenet\\anaconda3\n",
      "\n",
      "C:\\Users\\jenet\\AppData\\Roaming\\Python\\Python37\\site-packages\n",
      "C:\\Users\\jenet\\anaconda3\\lib\\site-packages\n",
      "C:\\Users\\jenet\\anaconda3\\lib\\site-packages\\win32\n",
      "C:\\Users\\jenet\\anaconda3\\lib\\site-packages\\win32\\lib\n",
      "C:\\Users\\jenet\\anaconda3\\lib\\site-packages\\Pythonwin\n",
      "C:\\Users\\jenet\\anaconda3\\lib\\site-packages\\IPython\\extensions\n",
      "C:\\Users\\jenet\\.ipython\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jenet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math, copy, sys\n",
    "for path in sys.path:\n",
    "    print(path)\n",
    "#sys.path.append('env/lib/python3.6/site-packages') #this line assumes you are using env\n",
    "\n",
    "import torch\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet') \n",
    "\n",
    "from MoveData import *\n",
    "from Transformer import *\n",
    "from TalkTrain import *\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Options(batchsize=16, device=torch.device(\"cpu\"), epochs=200, \n",
    "              lr=0.05, max_len = 25, save_path = 'saved/weights/transformer_example_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path = 'saved/examplepairs.json', opt=opt)\n",
    "emb_dim, n_layers, heads, dropout = 32, 2, 8, 0.1 \n",
    "chloe = Transformer(len(infield.vocab), len(outfield.vocab), emb_dim, n_layers, heads, dropout)\n",
    "chloe.load_state_dict(torch.load(opt.save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation\n",
    "\n",
    "The next cell uses a `while` loop to make the chloe continuously ask for the next input sentence. \n",
    "\n",
    "say \"hi\" to chloe\n",
    "\n",
    "When you want to turn off this cell and end the conversation, tell her \"bye chloe\", or click Kernel-> Interrupt\n",
    "\n",
    "Here is an example conversation:\n",
    "\n",
    "You > hi\n",
    "\n",
    "Chloe > hi ! , can i tell you a joke ?\n",
    "\n",
    "You > ok\n",
    "\n",
    "Chloe > how do french cats say thank you ?\n",
    "\n",
    "You > how?\n",
    "\n",
    "Chloe > meowci beaucoup !\n",
    "\n",
    "You > haha\n",
    "\n",
    "Chloe > thanks for laughing at my joke\n",
    "\n",
    "You > any more?\n",
    "\n",
    "Chloe > you will have to teach me\n",
    "\n",
    "You > are you alive?\n",
    "\n",
    "Chloe > depends on your definition of alive , are viruses alive ?\n",
    "\n",
    "You > i dont know\n",
    "\n",
    "Chloe > i dont know either\n",
    "\n",
    "You > ok bye\n",
    "\n",
    "Chloe > bye ttyl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You > bye chloe\n",
      "Chloe > bye ttyl\n",
      "\n"
     ]
    }
   ],
   "source": [
    " while True:\n",
    "    tell_chloe = input(\"You > \")\n",
    "    chloes_reply = talk_to_chloe(tell_chloe, chloe, opt, infield, outfield)\n",
    "    if (\"bye chloe\" in tell_chloe or \"bye ttyl\" in chloes_reply):\n",
    "        print('Chloe > '+ chloes_reply + '\\n')\n",
    "        break\n",
    "    else:\n",
    "        print('Chloe > '+ chloes_reply + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Now lets teach chloe a few new tricks, use your preferred text editor to open the file called *custompairs.json* that is included in the */saved* folder and add a few of your own conversation pairs to the list. \n",
    "\n",
    "For example, if you want chloe to say \"hi vicki\" when you say, \"hi i am vicki\", then add this line to *saved/pairs.json*\n",
    "\n",
    "{\"listen\": \"hi i am vicki\", \"reply\" : \"hi vicki\"}\n",
    "\n",
    "Be careful not to add blank lines to the json file, if you do so on accident, just put your cursor on the blank line and hit *backspace* to get rid of it. \n",
    "\n",
    "In the cell below, `data_iter` is a data loader object that gives you training data in the form of batches everytime you call it, `infield` and `outfield` are objects that store the relationship between the strings in Chloe's vocabulary with their indices, for the words Chloe expects to hear and the words Chloe expects to use in response. What do I mean by this? go to Insert and insert a cell below then run `infield.vocab.stoi`, you will see a dictionary of all the words Chloe expects to hear and each word's integer index. We need to recreate this vocabulary because by adding more lines of data, you probably have added some new vocab words that chloe must know. `opt` is a object of the options class that stores your preferences such as your learning rate (lr), path to where you want your neural network weights saved, etc. Run the cell below AFTER you have added your lines of new data to 'saved/pairs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocab size 742 output vocab size 1025\n"
     ]
    }
   ],
   "source": [
    "opt = Options(batchsize=16, device=torch.device(\"cpu\"), epochs=50, \n",
    "              lr=0.01, max_len = 25, save_path = 'saved/weights/transformer_custom_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path = 'saved/custompairs.json', opt=opt)\n",
    "print('input vocab size', len(infield.vocab), 'output vocab size', len(outfield.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x0000020C97572948>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             '.': 2,\n",
       "             '?': 3,\n",
       "             ',': 4,\n",
       "             'you': 5,\n",
       "             '!': 6,\n",
       "             'i': 7,\n",
       "             'what': 8,\n",
       "             'it': 9,\n",
       "             'no': 10,\n",
       "             '...': 11,\n",
       "             'dwight': 12,\n",
       "             'oh': 13,\n",
       "             'yeah': 14,\n",
       "             'okay': 15,\n",
       "             'that': 16,\n",
       "             'is': 17,\n",
       "             'hey': 18,\n",
       "             \"i'm\": 19,\n",
       "             'are': 20,\n",
       "             \"that's\": 21,\n",
       "             'the': 22,\n",
       "             'not': 23,\n",
       "             'right': 24,\n",
       "             'yes': 25,\n",
       "             'thank': 26,\n",
       "             'to': 27,\n",
       "             'we': 28,\n",
       "             'a': 29,\n",
       "             'do': 30,\n",
       "             'ok': 31,\n",
       "             'my': 32,\n",
       "             'nan': 33,\n",
       "             'on': 34,\n",
       "             'know': 35,\n",
       "             'me': 36,\n",
       "             \"don't\": 37,\n",
       "             'get': 38,\n",
       "             'all': 39,\n",
       "             'this': 40,\n",
       "             'was': 41,\n",
       "             'so': 42,\n",
       "             'about': 43,\n",
       "             'michael': 44,\n",
       "             'and': 45,\n",
       "             'god': 46,\n",
       "             'here': 47,\n",
       "             'hi': 48,\n",
       "             'be': 49,\n",
       "             'doing': 50,\n",
       "             'got': 51,\n",
       "             \"it's\": 52,\n",
       "             'stop': 53,\n",
       "             'am': 54,\n",
       "             'go': 55,\n",
       "             'good': 56,\n",
       "             'great': 57,\n",
       "             'in': 58,\n",
       "             'there': 59,\n",
       "             'well': 60,\n",
       "             'yep': 61,\n",
       "             'come': 62,\n",
       "             'did': 63,\n",
       "             'just': 64,\n",
       "             \"what's\": 65,\n",
       "             'can': 66,\n",
       "             'hello': 67,\n",
       "             'hmm': 68,\n",
       "             'how': 69,\n",
       "             'jim': 70,\n",
       "             'really': 71,\n",
       "             'sorry': 72,\n",
       "             'for': 73,\n",
       "             'gonna': 74,\n",
       "             'have': 75,\n",
       "             'him': 76,\n",
       "             'nice': 77,\n",
       "             'say': 78,\n",
       "             'think': 79,\n",
       "             \"'\": 80,\n",
       "             'pam': 81,\n",
       "             'scranton': 82,\n",
       "             'should': 83,\n",
       "             'too': 84,\n",
       "             'um': 85,\n",
       "             'who': 86,\n",
       "             \"you're\": 87,\n",
       "             'your': 88,\n",
       "             'fine': 89,\n",
       "             'going': 90,\n",
       "             'love': 91,\n",
       "             'see': 92,\n",
       "             'sure': 93,\n",
       "             'uh': 94,\n",
       "             'want': 95,\n",
       "             'at': 96,\n",
       "             'city': 97,\n",
       "             'cool': 98,\n",
       "             'electric': 99,\n",
       "             'exactly': 100,\n",
       "             'excuse': 101,\n",
       "             'guys': 102,\n",
       "             'idea': 103,\n",
       "             'like': 104,\n",
       "             'nothing': 105,\n",
       "             'of': 106,\n",
       "             'out': 107,\n",
       "             'please': 108,\n",
       "             'said': 109,\n",
       "             'take': 110,\n",
       "             'up': 111,\n",
       "             'wait': 112,\n",
       "             'whoa': 113,\n",
       "             'why': 114,\n",
       "             'with': 115,\n",
       "             'work': 116,\n",
       "             'absolutely': 117,\n",
       "             'alright': 118,\n",
       "             'big': 119,\n",
       "             'down': 120,\n",
       "             \"i'll\": 121,\n",
       "             'maybe': 122,\n",
       "             'thanks': 123,\n",
       "             \"we're\": 124,\n",
       "             'would': 125,\n",
       "             'angela': 126,\n",
       "             'anything': 127,\n",
       "             'aw': 128,\n",
       "             'back': 129,\n",
       "             \"didn't\": 130,\n",
       "             'feel': 131,\n",
       "             'halpert': 132,\n",
       "             \"he's\": 133,\n",
       "             'kay': 134,\n",
       "             'look': 135,\n",
       "             'na': 136,\n",
       "             'one': 137,\n",
       "             'oscar': 138,\n",
       "             'she': 139,\n",
       "             'shut': 140,\n",
       "             'talking': 141,\n",
       "             \"there's\": 142,\n",
       "             'three': 143,\n",
       "             'way': 144,\n",
       "             'were': 145,\n",
       "             'will': 146,\n",
       "             '..': 147,\n",
       "             'actually': 148,\n",
       "             'an': 149,\n",
       "             'call': 150,\n",
       "             \"can't\": 151,\n",
       "             'car': 152,\n",
       "             'christmas': 153,\n",
       "             'could': 154,\n",
       "             'croak': 155,\n",
       "             'does': 156,\n",
       "             'done': 157,\n",
       "             'enough': 158,\n",
       "             'ever': 159,\n",
       "             'five': 160,\n",
       "             'forget': 161,\n",
       "             'funny': 162,\n",
       "             'he': 163,\n",
       "             'her': 164,\n",
       "             \"i'd\": 165,\n",
       "             'kelly': 166,\n",
       "             'kevin': 167,\n",
       "             'let': 168,\n",
       "             \"let's\": 169,\n",
       "             'make': 170,\n",
       "             'merry': 171,\n",
       "             'mm': 172,\n",
       "             'movie': 173,\n",
       "             'mush': 174,\n",
       "             'name': 175,\n",
       "             'oui': 176,\n",
       "             'over': 177,\n",
       "             'people': 178,\n",
       "             'shh': 179,\n",
       "             'stay': 180,\n",
       "             'talk': 181,\n",
       "             'tell': 182,\n",
       "             'they': 183,\n",
       "             'where': 184,\n",
       "             'whoo': 185,\n",
       "             'win': 186,\n",
       "             'wow': 187,\n",
       "             'andy': 188,\n",
       "             'aye': 189,\n",
       "             'baby': 190,\n",
       "             'bad': 191,\n",
       "             'both': 192,\n",
       "             'boyfriend': 193,\n",
       "             'break': 194,\n",
       "             'but': 195,\n",
       "             'bye': 196,\n",
       "             'cards': 197,\n",
       "             'care': 198,\n",
       "             'check': 199,\n",
       "             'coming': 200,\n",
       "             'cute': 201,\n",
       "             'damn': 202,\n",
       "             'deal': 203,\n",
       "             'definitely': 204,\n",
       "             'different': 205,\n",
       "             \"doesn't\": 206,\n",
       "             'egregious': 207,\n",
       "             'eh': 208,\n",
       "             'else': 209,\n",
       "             'em': 210,\n",
       "             'everybody': 211,\n",
       "             'everything': 212,\n",
       "             'family': 213,\n",
       "             'forms': 214,\n",
       "             'from': 215,\n",
       "             'gosh': 216,\n",
       "             'guess': 217,\n",
       "             'happened': 218,\n",
       "             'hate': 219,\n",
       "             'hell': 220,\n",
       "             'help': 221,\n",
       "             'huh': 222,\n",
       "             'impossible': 223,\n",
       "             'jan': 224,\n",
       "             'jo': 225,\n",
       "             'karen': 226,\n",
       "             'keep': 227,\n",
       "             'kidding': 228,\n",
       "             'kill': 229,\n",
       "             'knock': 230,\n",
       "             'little': 231,\n",
       "             'live': 232,\n",
       "             'manager': 233,\n",
       "             'mom': 234,\n",
       "             'move': 235,\n",
       "             'much': 236,\n",
       "             'nellie': 237,\n",
       "             'nope': 238,\n",
       "             'now': 239,\n",
       "             'old': 240,\n",
       "             'or': 241,\n",
       "             'ow': 242,\n",
       "             'play': 243,\n",
       "             'probably': 244,\n",
       "             'question': 245,\n",
       "             'ryan': 246,\n",
       "             'scared': 247,\n",
       "             'seconds': 248,\n",
       "             \"she's\": 249,\n",
       "             'smart': 250,\n",
       "             'something': 251,\n",
       "             'son': 252,\n",
       "             'stanley': 253,\n",
       "             'stupid': 254,\n",
       "             'them': 255,\n",
       "             \"they're\": 256,\n",
       "             'top': 257,\n",
       "             'true': 258,\n",
       "             'trying': 259,\n",
       "             'two': 260,\n",
       "             'umm': 261,\n",
       "             'use': 262,\n",
       "             'very': 263,\n",
       "             'wanted': 264,\n",
       "             \"wasn't\": 265,\n",
       "             'whatever': 266,\n",
       "             'yet': 267,\n",
       "             '101': 268,\n",
       "             '120': 269,\n",
       "             '13': 270,\n",
       "             '17': 271,\n",
       "             '180': 272,\n",
       "             '200': 273,\n",
       "             '3rd': 274,\n",
       "             '49': 275,\n",
       "             'accident': 276,\n",
       "             'accounting': 277,\n",
       "             'addor': 278,\n",
       "             'after': 279,\n",
       "             'afwaid': 280,\n",
       "             'again': 281,\n",
       "             'aggendi': 282,\n",
       "             'agh': 283,\n",
       "             'ahead': 284,\n",
       "             'ahhh': 285,\n",
       "             'alert': 286,\n",
       "             'allowed': 287,\n",
       "             'already': 288,\n",
       "             'altoid': 289,\n",
       "             'always': 290,\n",
       "             'amen': 291,\n",
       "             'animals': 292,\n",
       "             'ankle': 293,\n",
       "             'another': 294,\n",
       "             'any': 295,\n",
       "             'anymore': 296,\n",
       "             'anyway': 297,\n",
       "             'appropriate': 298,\n",
       "             'argh': 299,\n",
       "             'around': 300,\n",
       "             'arrrgggh': 301,\n",
       "             'art': 302,\n",
       "             'as': 303,\n",
       "             'assistant': 304,\n",
       "             'associate': 305,\n",
       "             'awful': 306,\n",
       "             'ax': 307,\n",
       "             'bait': 308,\n",
       "             'barely': 309,\n",
       "             'bars': 310,\n",
       "             'beach': 311,\n",
       "             'because': 312,\n",
       "             'bed': 313,\n",
       "             'before': 314,\n",
       "             'being': 315,\n",
       "             'bernard': 316,\n",
       "             'better': 317,\n",
       "             'bill': 318,\n",
       "             'bird': 319,\n",
       "             'black': 320,\n",
       "             'blood': 321,\n",
       "             'bottom': 322,\n",
       "             'bought': 323,\n",
       "             'boy': 324,\n",
       "             'boys': 325,\n",
       "             'bring': 326,\n",
       "             'brother': 327,\n",
       "             'brothers': 328,\n",
       "             'bueno': 329,\n",
       "             \"building's\": 330,\n",
       "             'bullfrog': 331,\n",
       "             'bullpen': 332,\n",
       "             'buttlicker': 333,\n",
       "             'buyer': 334,\n",
       "             'buys': 335,\n",
       "             'c': 336,\n",
       "             'calling': 337,\n",
       "             'calms': 338,\n",
       "             'cannabis': 339,\n",
       "             \"cap'n\": 340,\n",
       "             'capping': 341,\n",
       "             'captain': 342,\n",
       "             'carol': 343,\n",
       "             'case': 344,\n",
       "             'cases': 345,\n",
       "             'casey': 346,\n",
       "             'cat': 347,\n",
       "             'chairs': 348,\n",
       "             'change': 349,\n",
       "             'chocolates': 350,\n",
       "             'choice': 351,\n",
       "             'choreographed': 352,\n",
       "             'cimmittee': 353,\n",
       "             'clark': 354,\n",
       "             'classic': 355,\n",
       "             'clean': 356,\n",
       "             'client': 357,\n",
       "             'clients': 358,\n",
       "             'climbing': 359,\n",
       "             'cliques': 360,\n",
       "             'closer': 361,\n",
       "             'coincidence': 362,\n",
       "             'collecting': 363,\n",
       "             'comin': 364,\n",
       "             'congratulations': 365,\n",
       "             'conned': 366,\n",
       "             'consider': 367,\n",
       "             'contact': 368,\n",
       "             'convenience': 369,\n",
       "             'cornbread': 370,\n",
       "             \"cornell's\": 371,\n",
       "             'cousin': 372,\n",
       "             'crazy': 373,\n",
       "             'credit': 374,\n",
       "             'crooked': 375,\n",
       "             'crotch': 376,\n",
       "             'crystal': 377,\n",
       "             'd': 378,\n",
       "             'da': 379,\n",
       "             'dancers': 380,\n",
       "             'dang': 381,\n",
       "             'danger': 382,\n",
       "             'date': 383,\n",
       "             'dave': 384,\n",
       "             'day': 385,\n",
       "             'dean': 386,\n",
       "             'delight': 387,\n",
       "             'depp': 388,\n",
       "             'difference': 389,\n",
       "             'ditto': 390,\n",
       "             'dogs': 391,\n",
       "             'donor': 392,\n",
       "             'door': 393,\n",
       "             'drawer': 394,\n",
       "             'drink': 395,\n",
       "             'driving': 396,\n",
       "             'drop': 397,\n",
       "             'drove': 398,\n",
       "             'dude': 399,\n",
       "             'dumbest': 400,\n",
       "             'dvds': 401,\n",
       "             'dwi': 402,\n",
       "             'eat': 403,\n",
       "             'eats': 404,\n",
       "             'egghead': 405,\n",
       "             'eight': 406,\n",
       "             'eighth': 407,\n",
       "             'emails': 408,\n",
       "             'emergency': 409,\n",
       "             'emgergency': 410,\n",
       "             'erin': 411,\n",
       "             'essential': 412,\n",
       "             'evel': 413,\n",
       "             'even': 414,\n",
       "             'everyone': 415,\n",
       "             'excellent': 416,\n",
       "             'eye': 417,\n",
       "             'eyeliner': 418,\n",
       "             'eyes': 419,\n",
       "             'failed': 420,\n",
       "             'famous': 421,\n",
       "             'fashioned': 422,\n",
       "             'fear': 423,\n",
       "             'firing': 424,\n",
       "             'first': 425,\n",
       "             'flattered': 426,\n",
       "             'floss': 427,\n",
       "             'forever': 428,\n",
       "             'formality': 429,\n",
       "             'foth': 430,\n",
       "             'four': 431,\n",
       "             'fredrick': 432,\n",
       "             'gay': 433,\n",
       "             'goes': 434,\n",
       "             'gone': 435,\n",
       "             'goodness': 436,\n",
       "             'gould': 437,\n",
       "             'gourmand': 438,\n",
       "             'gross': 439,\n",
       "             'grotti': 440,\n",
       "             'guest': 441,\n",
       "             'guests': 442,\n",
       "             'guten': 443,\n",
       "             'ha': 444,\n",
       "             'haaaww': 445,\n",
       "             'had': 446,\n",
       "             'haha': 447,\n",
       "             'happen': 448,\n",
       "             'happening': 449,\n",
       "             'harder': 450,\n",
       "             'has': 451,\n",
       "             'hear': 452,\n",
       "             'heard': 453,\n",
       "             'heck': 454,\n",
       "             'hellloo': 455,\n",
       "             'helping': 456,\n",
       "             \"here's\": 457,\n",
       "             'herr': 458,\n",
       "             'higher': 459,\n",
       "             'hilarious': 460,\n",
       "             'his': 461,\n",
       "             'hoh': 462,\n",
       "             'home': 463,\n",
       "             'hot': 464,\n",
       "             'hour': 465,\n",
       "             'hours': 466,\n",
       "             \"how's\": 467,\n",
       "             'howard': 468,\n",
       "             'human': 469,\n",
       "             'humanly': 470,\n",
       "             'hundred': 471,\n",
       "             'hurt': 472,\n",
       "             'icks': 473,\n",
       "             'id': 474,\n",
       "             'im': 475,\n",
       "             'implying': 476,\n",
       "             'important': 477,\n",
       "             'inbwit': 478,\n",
       "             'incredible': 479,\n",
       "             'indica': 480,\n",
       "             'infertility': 481,\n",
       "             'insulting': 482,\n",
       "             'integrity': 483,\n",
       "             'intimidate': 484,\n",
       "             'into': 485,\n",
       "             'involves': 486,\n",
       "             \"it'll\": 487,\n",
       "             \"jan's\": 488,\n",
       "             'jealousies': 489,\n",
       "             'jimbo': 490,\n",
       "             'job': 491,\n",
       "             'jobs': 492,\n",
       "             'johnny': 493,\n",
       "             'kgb': 494,\n",
       "             'kickin': 495,\n",
       "             'kid': 496,\n",
       "             'kind': 497,\n",
       "             'knee': 498,\n",
       "             'knew': 499,\n",
       "             'knievel': 500,\n",
       "             'knows': 501,\n",
       "             'kobayashi': 502,\n",
       "             'later': 503,\n",
       "             'lattes': 504,\n",
       "             'least': 505,\n",
       "             'leaving': 506,\n",
       "             'liar': 507,\n",
       "             'lie': 508,\n",
       "             'life': 509,\n",
       "             'lights': 510,\n",
       "             'liked': 511,\n",
       "             'lines': 512,\n",
       "             'load': 513,\n",
       "             'lock': 514,\n",
       "             'looking': 515,\n",
       "             'lost': 516,\n",
       "             'lot': 517,\n",
       "             'loud': 518,\n",
       "             'loyalty': 519,\n",
       "             'lunch': 520,\n",
       "             'lying': 521,\n",
       "             'man': 522,\n",
       "             'marijuana': 523,\n",
       "             'market': 524,\n",
       "             'maru': 525,\n",
       "             'mate': 526,\n",
       "             'matter': 527,\n",
       "             'matthews': 528,\n",
       "             'may': 529,\n",
       "             'mean': 530,\n",
       "             'medal': 531,\n",
       "             'meet': 532,\n",
       "             'meeting': 533,\n",
       "             'meh': 534,\n",
       "             'mema': 535,\n",
       "             'milkshake': 536,\n",
       "             'mind': 537,\n",
       "             'minutes': 538,\n",
       "             'miss': 539,\n",
       "             'mister': 540,\n",
       "             'mmm': 541,\n",
       "             'more': 542,\n",
       "             'morning': 543,\n",
       "             'mose': 544,\n",
       "             'mother': 545,\n",
       "             'moves': 546,\n",
       "             'moving': 547,\n",
       "             'must': 548,\n",
       "             'muy': 549,\n",
       "             'need': 550,\n",
       "             'negative': 551,\n",
       "             'neither': 552,\n",
       "             'nerd': 553,\n",
       "             'never': 554,\n",
       "             'new': 555,\n",
       "             'news': 556,\n",
       "             'next': 557,\n",
       "             'night': 558,\n",
       "             'nines': 559,\n",
       "             'nn': 560,\n",
       "             'nobody': 561,\n",
       "             'northern': 562,\n",
       "             'northernmost': 563,\n",
       "             'noted': 564,\n",
       "             'o': 565,\n",
       "             'obvious': 566,\n",
       "             'off': 567,\n",
       "             'ohhhh': 568,\n",
       "             \"old's\": 569,\n",
       "             \"one's\": 570,\n",
       "             'only': 571,\n",
       "             'oof': 572,\n",
       "             'ooh': 573,\n",
       "             'ooooh': 574,\n",
       "             'open': 575,\n",
       "             'optional': 576,\n",
       "             \"oscar's\": 577,\n",
       "             'other': 578,\n",
       "             'our': 579,\n",
       "             'outside': 580,\n",
       "             'oven': 581,\n",
       "             'packed': 582,\n",
       "             'painted': 583,\n",
       "             \"pam's\": 584,\n",
       "             'participating': 585,\n",
       "             'party': 586,\n",
       "             'percent': 587,\n",
       "             'pervert': 588,\n",
       "             'pff': 589,\n",
       "             \"phone's\": 590,\n",
       "             'picture': 591,\n",
       "             'pink': 592,\n",
       "             'place': 593,\n",
       "             'planning': 594,\n",
       "             'pleasure': 595,\n",
       "             'poop': 596,\n",
       "             'pop': 597,\n",
       "             'possible': 598,\n",
       "             'practice': 599,\n",
       "             'pregnant': 600,\n",
       "             'presbyterian': 601,\n",
       "             'presents': 602,\n",
       "             'president': 603,\n",
       "             'promise': 604,\n",
       "             'pros': 605,\n",
       "             'protein': 606,\n",
       "             'punch': 607,\n",
       "             'push': 608,\n",
       "             'quiet': 609,\n",
       "             'r': 610,\n",
       "             'rainbows': 611,\n",
       "             'ready': 612,\n",
       "             'real': 613,\n",
       "             'reasons': 614,\n",
       "             'red': 615,\n",
       "             'regional': 616,\n",
       "             'resolution': 617,\n",
       "             'retired': 618,\n",
       "             'ribs': 619,\n",
       "             'ridiculous': 620,\n",
       "             'ringing': 621,\n",
       "             'rogaine': 622,\n",
       "             'room': 623,\n",
       "             'round': 624,\n",
       "             'roxbury': 625,\n",
       "             'rule': 626,\n",
       "             'running': 627,\n",
       "             'sad': 628,\n",
       "             'safe': 629,\n",
       "             'same': 630,\n",
       "             'saying': 631,\n",
       "             'school': 632,\n",
       "             'scott': 633,\n",
       "             'scream': 634,\n",
       "             'screaming': 635,\n",
       "             'screwed': 636,\n",
       "             'seduce': 637,\n",
       "             'sense': 638,\n",
       "             'sensei': 639,\n",
       "             'serious': 640,\n",
       "             'seven': 641,\n",
       "             'shark': 642,\n",
       "             'shi': 643,\n",
       "             'shoot': 644,\n",
       "             \"shouldn't\": 645,\n",
       "             'show': 646,\n",
       "             'showtime': 647,\n",
       "             'si': 648,\n",
       "             'side': 649,\n",
       "             'significant': 650,\n",
       "             'silver': 651,\n",
       "             'sit': 652,\n",
       "             'six': 653,\n",
       "             'sleep': 654,\n",
       "             'slytherin': 655,\n",
       "             'small': 656,\n",
       "             'smell': 657,\n",
       "             'snoot': 658,\n",
       "             'some': 659,\n",
       "             'somebody': 660,\n",
       "             'someone': 661,\n",
       "             'sort': 662,\n",
       "             'sounds': 663,\n",
       "             'spoiler': 664,\n",
       "             'spy': 665,\n",
       "             'stairs': 666,\n",
       "             'standard': 667,\n",
       "             'steps': 668,\n",
       "             'sticking': 669,\n",
       "             'still': 670,\n",
       "             'streamers': 671,\n",
       "             'strongly': 672,\n",
       "             'struck': 673,\n",
       "             'such': 674,\n",
       "             'suck': 675,\n",
       "             'sucker': 676,\n",
       "             'sun': 677,\n",
       "             'suppose': 678,\n",
       "             'sweet': 679,\n",
       "             'tag': 680,\n",
       "             'taking': 681,\n",
       "             'team': 682,\n",
       "             'temporary': 683,\n",
       "             'ten': 684,\n",
       "             'theme': 685,\n",
       "             'these': 686,\n",
       "             'thing': 687,\n",
       "             'thinking': 688,\n",
       "             'thirty': 689,\n",
       "             'thought': 690,\n",
       "             'thriller': 691,\n",
       "             'tie': 692,\n",
       "             'tip': 693,\n",
       "             'toast': 694,\n",
       "             'toaster': 695,\n",
       "             'toby': 696,\n",
       "             'today': 697,\n",
       "             'tornadoes': 698,\n",
       "             'totally': 699,\n",
       "             'touch': 700,\n",
       "             'touching': 701,\n",
       "             'toward': 702,\n",
       "             'trains': 703,\n",
       "             'trouble': 704,\n",
       "             'truce': 705,\n",
       "             'try': 706,\n",
       "             'tv': 707,\n",
       "             'twenty': 708,\n",
       "             'underground': 709,\n",
       "             'universal': 710,\n",
       "             'until': 711,\n",
       "             'upstairs': 712,\n",
       "             'urine': 713,\n",
       "             'used': 714,\n",
       "             'vine': 715,\n",
       "             'vote': 716,\n",
       "             'vouched': 717,\n",
       "             'wearing': 718,\n",
       "             'wedding': 719,\n",
       "             'weird': 720,\n",
       "             'welcome': 721,\n",
       "             'whack': 722,\n",
       "             'whass': 723,\n",
       "             'whhh': 724,\n",
       "             'wilhelm': 725,\n",
       "             'winking': 726,\n",
       "             'word': 727,\n",
       "             'worker': 728,\n",
       "             'working': 729,\n",
       "             'workspace': 730,\n",
       "             'worth': 731,\n",
       "             'wound': 732,\n",
       "             'wrong': 733,\n",
       "             'wrote': 734,\n",
       "             'x': 735,\n",
       "             'yah': 736,\n",
       "             'yawn': 737,\n",
       "             'yo': 738,\n",
       "             'yourself': 739,\n",
       "             'yup': 740,\n",
       "             'z': 741})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infield.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model\n",
    "\n",
    "OK, now that we have built a data loader, a vocabulary and an object to store our preferences, lets instantiate a Transformer sequence to sequence model. There is alot summoned by the line `model = Transformer(len(infield.vocab), len(outfield.vocab), emb_dim, n_layers, heads, dropout)`, Transformers are the general neural architecture behind many of hyped up / notorious research models of 2017-2019 such as OpenAI's [GPT-2](http://jalammar.github.io/illustrated-gpt2/) and Google AI's [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "We will take our time with dissecting and understanding it's components later. In the cell below `emb_dim`, `n_layers`, `heads`, `dropout` stand for embedding dimensions, number of layers, attention heads and dropout. These are some of the specifications that indicate the size and complexity of the Transformer we are going to instantiate. The number provided here create a relatively small Transformer for our toy example. `model` is the instance of the Transformer, aka chloe, that we are creating and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim, n_layers, heads, dropout = 16, 8, 8, 0.1 \n",
    "chloe = Transformer(len(infield.vocab), len(outfield.vocab), emb_dim, n_layers, heads, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Neural network optimization is a whole [field](https://www.jeremyjordan.me/nn-learning-rate/) in itself, we will talk more about this in the future. For now, just know the learning rate `opt.lr` is a hyperparameter whose initial value we choose, it modifies the magnitude of the step the Adam optimizer algorithm will take to update the weights, aka parameters, of the neural network model during training. As training progresses the learning rate is also changing according to a scheduler that monitors the learning progress. `epochs` is the number of times we will cycle through the data during training. If you trained on the same dataset in a different sitting and would like to reload that trained model instead of training from scratch, simply paste this line of code below `model.load_state_dict(torch.load(opt.save_path))` before running it. The cell below defines the learning rate, epochs, type of optimzer and type of scheduler we will use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train the chloe on your modified json dataset, chloe should quickly memorize the data. As the loss decreases, chloe learns from the data to output the corresponding sequence when fed inputs that are close enough to the training inputs. When the loss is less than 0.1, the responses should become coherent. You can re-instantiate chloe to start fresh or rerun the cell below if you need to get the loss lower with more training. If the loss is not yet less than 0.1, just run the cell below again and train from where you left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, data_iterator, options, optimizer, scheduler):\n",
    "\n",
    "    if torch.cuda.is_available() and options.device == torch.device(\"cuda:0\"):\n",
    "        print(\"a GPU was detected, model will be trained on GPU\")\n",
    "        #model = model.cuda()\n",
    "        model = model.to(torch.device('cuda:0'))\n",
    "    else:\n",
    "        print(\"training on cpu\")\n",
    "\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    best_loss = 100\n",
    "    for epoch in range(options.epochs):\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(data_iterator): \n",
    "            src = batch.listen.transpose(0,1)\n",
    "            trg = batch.reply.transpose(0,1)\n",
    "            #print(trg)\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, options)\n",
    "            preds = model(src, src_mask, trg_input, trg_mask)\n",
    "            #print(preds.shape, trg.shape)\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            optimizer.zero_grad()\n",
    "            preds = preds.view(-1, preds.size(-1))\n",
    "            #print(preds.shape, ys.shape)\n",
    "            batch_loss = F.cross_entropy(preds, ys, \n",
    "                                         ignore_index = options.trg_pad)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "        epoch_loss = total_loss/(num_batches(data_iterator)+1)\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            print(f'saving model at', options.save_path)\n",
    "            torch.save(model.state_dict(), options.save_path)\n",
    "            \n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, epoch, epoch_loss))\n",
    "        total_loss = 0\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n",
      "saving model at saved/weights/transformer_custom_weights\n",
      "0m: epoch 0 loss = 0.725\n",
      "saving model at saved/weights/transformer_custom_weights\n",
      "0m: epoch 1 loss = 0.719\n",
      "0m: epoch 2 loss = 0.727\n",
      "saving model at saved/weights/transformer_custom_weights\n",
      "0m: epoch 3 loss = 0.707\n",
      "0m: epoch 4 loss = 0.722\n",
      "saving model at saved/weights/transformer_custom_weights\n",
      "0m: epoch 5 loss = 0.703\n",
      "saving model at saved/weights/transformer_custom_weights\n",
      "0m: epoch 6 loss = 0.692\n",
      "1m: epoch 7 loss = 0.710\n",
      "1m: epoch 8 loss = 0.723\n",
      "1m: epoch 9 loss = 0.711\n",
      "1m: epoch 10 loss = 0.727\n",
      "1m: epoch 11 loss = 0.731\n",
      "1m: epoch 12 loss = 0.720\n",
      "1m: epoch 13 loss = 0.714\n",
      "2m: epoch 14 loss = 0.700\n",
      "2m: epoch 15 loss = 0.717\n",
      "2m: epoch 16 loss = 0.702\n",
      "2m: epoch 17 loss = 0.726\n",
      "2m: epoch 18 loss = 0.714\n",
      "2m: epoch 19 loss = 0.711\n",
      "2m: epoch 20 loss = 0.713\n",
      "2m: epoch 21 loss = 0.705\n",
      "3m: epoch 22 loss = 0.717\n",
      "3m: epoch 23 loss = 0.713\n",
      "3m: epoch 24 loss = 0.726\n",
      "3m: epoch 25 loss = 0.709\n",
      "3m: epoch 26 loss = 0.700\n",
      "3m: epoch 27 loss = 0.737\n",
      "3m: epoch 28 loss = 0.733\n",
      "4m: epoch 29 loss = 0.711\n",
      "4m: epoch 30 loss = 0.718\n",
      "4m: epoch 31 loss = 0.716\n",
      "4m: epoch 32 loss = 0.728\n",
      "4m: epoch 33 loss = 0.711\n",
      "4m: epoch 34 loss = 0.711\n",
      "4m: epoch 35 loss = 0.728\n",
      "4m: epoch 36 loss = 0.711\n",
      "5m: epoch 37 loss = 0.708\n",
      "5m: epoch 38 loss = 0.701\n",
      "5m: epoch 39 loss = 0.711\n",
      "5m: epoch 40 loss = 0.711\n",
      "5m: epoch 41 loss = 0.717\n",
      "5m: epoch 42 loss = 0.703\n",
      "5m: epoch 43 loss = 0.717\n",
      "6m: epoch 44 loss = 0.720\n",
      "6m: epoch 45 loss = 0.701\n",
      "6m: epoch 46 loss = 0.718\n",
      "6m: epoch 47 loss = 0.729\n",
      "6m: epoch 48 loss = 0.708\n",
      "6m: epoch 49 loss = 0.697\n"
     ]
    }
   ],
   "source": [
    "chloe = trainer(chloe, data_iter, opt, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now talk to chloe! by modifying the `tell_chloe` variable and running the cell below. Your input sentence that Chloe hears has to be tokenized (split into separate words), converted from strings to a sequence of integers, inputted to the model (chloe), who then responds with a sequence of integers, that sequence is converted back into strings for you to read. All this is taken care of by the `talk_to_model()` function below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chloe > would you like the stink sack ?\n",
      "\n",
      "Chloe > do you ever ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tell_chloe = \"hi i am vicki\" \n",
    "chloes_reply = talk_to_chloe(tell_chloe, chloe, opt, infield, outfield)\n",
    "print('Chloe > '+ chloes_reply + '\\n')\n",
    "tell_chloe = \"Are you a robot\"\n",
    "chloes_reply = talk_to_chloe(tell_chloe, chloe, opt, infield, outfield)\n",
    "print('Chloe > '+ chloes_reply + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Notice that chloe is a combination of hard coded rules and also neural network. The neural network portion allows chloe to encode, or represent, your messages to her in a way that she can make use of, even if that message was not exactly in the training data. if it is close enough, she knows what to do next. chloe is cute, at least i think so, but there is alot we can do to make chloe smarter and more useful. \n",
    "\n",
    "For example, is chloe just responding to each of your messages with a simple mapping between input and output? or does chloe take into account the entire conversation so far, or even previous conversations? is chloe trying to accomplish anything? what is the point of her conversation?  is there a reward signal we can build into the learning so that chloebot learns from experience to achieve a goal aka objective? can chloe learn new words or understand misspelled words? not yet. can chloebot use outside knowledge to inform her conversations? not yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You > What are we going to do?\n",
      "Chloe > hurl your feces .\n",
      "\n",
      "You > Hi Dwight\n",
      "Chloe > merry christmas . can i saying .\n",
      "\n",
      "You > What was the job?\n",
      "Chloe > i'm fine .\n",
      "\n",
      "You > I have ten dollars\n",
      "Chloe > nametag ?\n",
      "\n",
      "You > How are you?\n",
      "Chloe > i'm all universal donor .\n",
      "\n",
      "You > hello\n",
      "Chloe > hiii !\n",
      "\n",
      "You > jim is listening to us?\n",
      "Chloe > do you have what're wearing eyeliner , say aphrodisiac .\n",
      "\n",
      "You > bye chloe\n",
      "Chloe > good .\n",
      "\n"
     ]
    }
   ],
   "source": [
    " while True:\n",
    "    tell_chloe = input(\"You > \")\n",
    "    chloes_reply = talk_to_chloe(tell_chloe, chloe, opt, infield, outfield)\n",
    "    if (\"bye chloe\" in tell_chloe or \"bye ttyl\" in chloes_reply):\n",
    "        print('Chloe > '+ chloes_reply + '\\n')\n",
    "        break\n",
    "    else:\n",
    "        print('Chloe > '+ chloes_reply + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whats Next\n",
    "\n",
    "The next lesson is an intuitive explaination of loss functions with a toy coding example that is expanded to pytorch tensors, tokenization and an explaination of the training function `trainer()` used in this introductory lesson. Go to `notebooks/Trainer.ipynb` for the next part of this adventure. see you there!\n",
    "\n",
    "<img src=\"https://avatars3.githubusercontent.com/u/56938552?s=100&v=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
